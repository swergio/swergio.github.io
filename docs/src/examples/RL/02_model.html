<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>02_model.py &mdash; swergio 0.0.1 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="03_aggregation.py" href="03_aggregation.html" />
    <link rel="prev" title="01_server.py" href="01_server.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> swergio
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../swergio/modules.html">Swergio Project</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../modules.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../Trebuchet/overview.html">Trebuchet</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="overview.html">Reinforcement Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="overview.html#requirements">Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#how-to-run-it">How to run it</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="overview.html#details">Details</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="00_introduction.html">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="01_server.html">01_server.py</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">02_model.py</a></li>
<li class="toctree-l4"><a class="reference internal" href="03_aggregation.html">03_aggregation.py</a></li>
<li class="toctree-l4"><a class="reference internal" href="04_gym_env.html">04_gym_env.py</a></li>
<li class="toctree-l4"><a class="reference internal" href="05_evolutionary.html">05_evolutionary.py</a></li>
<li class="toctree-l4"><a class="reference internal" href="99_swarm.html">Set up the swarm</a></li>
<li class="toctree-l4"><a class="reference internal" href="06_control.html">06_control.py</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../MelodyCreator/overview.html">Melody Creator</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../package_apis/modules.html">Packages API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">swergio</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../modules.html">Examples</a></li>
          <li class="breadcrumb-item"><a href="overview.html">Reinforcement Learning</a></li>
      <li class="breadcrumb-item active">02_model.py</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/src/examples/RL/02_model.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="model-py">
<h1>02_model.py<a class="headerlink" href="#model-py" title="Permalink to this heading"></a></h1>
<p>We will now create a <code class="docutils literal notranslate"><span class="pre">02_model.py</span></code> file, which contains the logic for reinforcement learning model using a A2C policy.
The model will receive the current observation from the environment and proposed distribution over the action space, which the aggregation component will use to determine the final action.
To simplify the implementation of the RL model we use parts from the stable_baselines3 package including the model definition (A2C) and the RolloutBuffer to train the model.
Every time the component gets new feedback regarding actual taken action and given reward, we store these values in the rollout buffer. Once the buffer is full, the policy is trained on the new information.</p>
<p>Additional we enable this component to communicate and update the models weights with other swergio components. This enables us to run multiple models at the same time and easily adjust the model weights with an external component e.g. an evolutionary algorithm.</p>
<p>Let’s first import all necessary packages.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">import</span> <span class="nn">socket</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.policies</span> <span class="kn">import</span> <span class="n">ActorCriticPolicy</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.buffers</span> <span class="kn">import</span>  <span class="n">RolloutBuffer</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.utils</span> <span class="kn">import</span>  <span class="n">get_schedule_fn</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">swergio</span>  <span class="kn">import</span> <span class="n">Client</span><span class="p">,</span> <span class="n">Trigger</span><span class="p">,</span> <span class="n">MESSAGE_TYPE</span>
<span class="kn">from</span> <span class="nn">swergio_toolbox.objects</span> <span class="kn">import</span> <span class="n">MutableBool</span>
</pre></div>
</div>
<p>First we set the component name to <em>model</em>, which is use in the communication.
Additional we add a unique component ID which will be generated once the scrip is executed. This allows us to run multiple models in parallel while each having a unique identifier.</p>
<p>We also need to specify the IP and the port of the server as well as the message format and the header length.
All of these information have to stay the same across the server and all clients.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">COMPONENT_NAME</span> <span class="o">=</span> <span class="s1">&#39;model&#39;</span>
<span class="n">COMPONENT_ID</span> <span class="o">=</span> <span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span>
<span class="n">PORT</span> <span class="o">=</span> <span class="mi">8080</span>
<span class="n">SERVER</span> <span class="o">=</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostbyname</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">())</span>
<span class="n">FORMAT</span> <span class="o">=</span> <span class="s1">&#39;utf-8&#39;</span>
<span class="n">HEADER_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>We can now define the RL model using stable_baselines3 methods.
First we have to define the observation and action space. These values are dependent on the gym environment we are using.
In this case the <em>CartPole-v1</em>.
We can then create our policy class including our desired learning rate and the architecture of the used policy network.
Last we set up the RolloutBuffer to store the observation, action, reward information for our training.
In this example we choose the size of the buffer to be 5, which means we will train our policy every 5 steps in the environment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">OBS_SPACE</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.8000002e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4028235e+38</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1887903e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4028235e+38</span><span class="p">]),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.8000002e+00</span><span class="p">,</span> <span class="mf">3.4028235e+38</span><span class="p">,</span> <span class="mf">4.1887903e-01</span><span class="p">,</span> <span class="mf">3.4028235e+38</span><span class="p">]),</span>
    <span class="p">(</span><span class="mi">4</span><span class="p">,),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ACT_SPACE</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">ActorCriticPolicy</span><span class="p">(</span><span class="n">observation_space</span><span class="o">=</span><span class="n">OBS_SPACE</span><span class="p">,</span> <span class="n">action_space</span> <span class="o">=</span> <span class="n">ACT_SPACE</span><span class="p">,</span>
                        <span class="n">lr_schedule</span><span class="o">=</span><span class="n">get_schedule_fn</span><span class="p">(</span><span class="mf">7e-4</span><span class="p">),</span>
                        <span class="n">net_arch</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">pi</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">vf</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])])</span>
<span class="n">rolloutbuffer</span> <span class="o">=</span> <span class="n">RolloutBuffer</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">observation_space</span><span class="o">=</span> <span class="n">OBS_SPACE</span><span class="p">,</span> <span class="n">action_space</span><span class="o">=</span> <span class="n">ACT_SPACE</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Additional to the RL model we define an internal memory as dictionary. This is used to store and remember information from one step to the other, e.g. prior observation we will have to link to a reward we receive later.
We also define a boolean variable to flag if training is enabled or not. Since we will change the value of this bool in the handler function we’ll use th custom MutableBool class.
Lastly we create the swergio Client by passing the required settings (NAME, SERVER, PORT etc. ) as well as the prior defined objects as keyword arguments, so they can be refereed to in our handling functions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">memory</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">training_enabled</span> <span class="o">=</span> <span class="n">MutableBool</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">COMPONENT_NAME</span><span class="p">,</span> <span class="n">SERVER</span><span class="p">,</span><span class="n">PORT</span><span class="p">,</span><span class="n">FORMAT</span><span class="p">,</span><span class="n">HEADER_LENGTH</span><span class="p">,</span><span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> <span class="n">rolloutbuffer</span> <span class="o">=</span><span class="n">rolloutbuffer</span><span class="p">,</span><span class="n">memory</span> <span class="o">=</span> <span class="n">memory</span><span class="p">,</span> <span class="n">training_enabled</span><span class="o">=</span><span class="n">training_enabled</span><span class="p">)</span>
</pre></div>
</div>
<p>Lets first define our training function, which will update the  policy based on the history in the rollout buffer.
For our training we will loop through each entry in the rollout buffer, calculate the policy, value and entropy loss and update our pytorch model in regard to the total loss.
To fine-tune our training we can adjust some hyperparameter if required as the value loss coefficien (<em>vf_coef</em>), the entropy loss coefficient (<em>ent_coef</em>) or the maximum grad_norm.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">rolloutbuffer</span><span class="p">):</span>
    <span class="n">ent_coef</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">vf_coef</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">max_grad_norm</span> <span class="o">=</span>  <span class="mf">0.5</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">set_training_mode</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">rollout_data</span> <span class="ow">in</span> <span class="n">rolloutbuffer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="o">.</span><span class="n">actions</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">values</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">evaluate_actions</span><span class="p">(</span><span class="n">rollout_data</span><span class="o">.</span><span class="n">observations</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="o">.</span><span class="n">advantages</span>
        <span class="c1"># Policy gradient loss</span>
        <span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">log_prob</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="c1"># Value loss using the TD(gae_lambda) target</span>
        <span class="n">value_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">rollout_data</span><span class="o">.</span><span class="n">returns</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
        <span class="c1"># Entropy loss favor exploration</span>
        <span class="k">if</span> <span class="n">entropy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Approximate entropy when no analytical form</span>
            <span class="n">entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">log_prob</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">entropy</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">policy_loss</span> <span class="o">+</span> <span class="n">ent_coef</span> <span class="o">*</span> <span class="n">entropy_loss</span> <span class="o">+</span> <span class="n">vf_coef</span> <span class="o">*</span> <span class="n">value_loss</span>
        <span class="c1"># Optimization step</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># Clip grad norm</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_grad_norm</span><span class="p">)</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">rolloutbuffer</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
<p>We can now add the event handler function to handle new messages from the environment. These functions are executed when we receive a certain type of message.</p>
<p>We first define the function that will handle our received messages.
Such a function requires the message (<em>msg</em>) as first parameter, which contains the all message information as dictionary.
Additional we can add arguments for the other objects we use in the functions (e.g. policy, rolloutbuffer, memory). The naming needs to be the same as the kwargs of the client.</p>
<p>The forward function of our component will receive the message including an OBSERVATION, the last ACTION took as well as the REWARD, a flag if the episode of the environment is DONE and  a flag if our action should be DETERMINISTIC.
In case our roll out buffer is full, we will first train our policy with the data in the roll out buffer.
We will then save the latest information to the rollout buffer, including th prior observation, estimated value and log_probs we stored in our memory from  the step before.
We cam then evaluate the current observation state and determine the value and action logits of our policy.
After saving these infos in our internal memory, we then add the logits of our action network including the unique COMPONENT_ID to message we will send to the aggregation component.</p>
<p>Finally we add a new event handler to our client object.
This includes the defined function that is executed when the handler is active as well as the MESSAGE_TYPE and response ROOM of our response message. In this case our response will be a DATA.CUSTOM type to the <em>logits</em> room.
We also need to set the Trigger to define which incoming messages the handler needs to process. In this case we will react to messages of type DATA.CUSTOM in the <em>observation</em> room.</p>
<p>Once added the event handler, the client will be added to the message rooms we require.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span><span class="n">policy</span><span class="p">,</span><span class="n">rolloutbuffer</span><span class="p">,</span> <span class="n">memory</span><span class="p">):</span>
    <span class="c1"># Train policy</span>
    <span class="k">if</span> <span class="n">rolloutbuffer</span><span class="o">.</span><span class="n">full</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">training_enabled</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
            <span class="n">rolloutbuffer</span><span class="o">.</span><span class="n">compute_returns_and_advantage</span><span class="p">(</span><span class="n">last_values</span><span class="o">=</span><span class="n">memory</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">],</span> <span class="n">dones</span><span class="o">=</span><span class="n">msg</span><span class="p">[</span><span class="s1">&#39;DONE&#39;</span><span class="p">]</span> <span class="k">if</span> <span class="s1">&#39;DONE&#39;</span> <span class="ow">in</span> <span class="n">msg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">train</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">rolloutbuffer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rolloutbuffer</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="c1"># Save to buffer</span>
    <span class="k">if</span> <span class="s1">&#39;REWARD&#39;</span> <span class="ow">in</span> <span class="n">msg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">msg</span><span class="p">[</span><span class="s1">&#39;REWARD&#39;</span><span class="p">]</span>
        <span class="n">rolloutbuffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">memory</span><span class="p">[</span><span class="s1">&#39;obs&#39;</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">msg</span><span class="p">[</span><span class="s1">&#39;ACTION&#39;</span><span class="p">]),</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;last_episode_starts&#39;</span><span class="p">],</span> <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">],</span> <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">])</span>
    <span class="n">obs</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">msg</span><span class="p">[</span><span class="s2">&quot;OBSERVATION&quot;</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Convert to pytorch tensor or to TensorDict</span>
        <span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="n">actions</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">obs_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">deterministic</span> <span class="o">=</span> <span class="n">msg</span><span class="p">[</span><span class="s1">&#39;DETERMINISTIC&#39;</span><span class="p">])</span>
        <span class="c1"># Preprocess the observation if needed</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">extract_features</span><span class="p">(</span><span class="n">obs_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">latent_pi</span><span class="p">,</span> <span class="n">latent_vf</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">mlp_extractor</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="c1"># Evaluate the values for the given observations</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">value_net</span><span class="p">(</span><span class="n">latent_vf</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_net</span><span class="p">(</span><span class="n">latent_pi</span><span class="p">)</span>
    <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">obs</span>
    <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;actions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;last_episode_starts&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">msg</span><span class="p">[</span><span class="s1">&#39;DONE&#39;</span><span class="p">]</span> <span class="k">if</span> <span class="s1">&#39;DONE&#39;</span> <span class="ow">in</span> <span class="n">msg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="kc">True</span>
    <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">memory</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;LOGITS&quot;</span><span class="p">:</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="s2">&quot;COMPONENT_ID&quot;</span><span class="p">:</span> <span class="n">COMPONENT_ID</span><span class="p">,</span> <span class="s2">&quot;DETERMINISTIC&quot;</span><span class="p">:</span><span class="n">msg</span><span class="p">[</span><span class="s1">&#39;DETERMINISTIC&#39;</span><span class="p">]}</span>
<span class="n">client</span><span class="o">.</span><span class="n">add_eventHandler</span><span class="p">(</span><span class="n">forward</span><span class="p">,</span><span class="n">MESSAGE_TYPE</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CUSTOM</span><span class="p">,</span><span class="n">responseRooms</span><span class="o">=</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span><span class="n">trigger</span><span class="o">=</span><span class="n">Trigger</span><span class="p">(</span><span class="n">MESSAGE_TYPE</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CUSTOM</span><span class="p">,</span> <span class="s1">&#39;observation&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>In our example we want to be able to change the model weights no just by training the policy, but also to update the model based on an evolutionary algorithm.
This evolutionary component is running separately and will just communicate the weights via message.
To enable this functionality here in our model component, we will first define two helper functions that can convert a pytorch model weights from and to a list/vector.</p>
<p>The first function <code class="docutils literal notranslate"><span class="pre">model_weights_as_vector</span></code> takes a pytorch model and provides the models weights as list.
The second function <code class="docutils literal notranslate"><span class="pre">model_weights_as_dict</span></code> requires a define pytorch model and the weights as list and will return a dictionary of the weights, that we can use to load into the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_weights_as_vector</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">weights_vector</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">curr_weights</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">curr_weights</span> <span class="o">=</span> <span class="n">curr_weights</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">curr_weights</span><span class="p">,</span> <span class="n">newshape</span><span class="o">=</span><span class="p">(</span><span class="n">curr_weights</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
        <span class="n">weights_vector</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights_vector</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_weights_as_dict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">weights_vector</span><span class="p">):</span>
    <span class="n">weights_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">weights_dict</span><span class="p">:</span>
        <span class="n">w_matrix</span> <span class="o">=</span> <span class="n">weights_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">layer_weights_shape</span> <span class="o">=</span> <span class="n">w_matrix</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">layer_weights_size</span> <span class="o">=</span> <span class="n">w_matrix</span><span class="o">.</span><span class="n">size</span>
        <span class="n">layer_weights_vector</span> <span class="o">=</span> <span class="n">weights_vector</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">start</span> <span class="o">+</span> <span class="n">layer_weights_size</span><span class="p">]</span>
        <span class="n">layer_weights_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">layer_weights_vector</span><span class="p">,</span> <span class="n">newshape</span><span class="o">=</span><span class="p">(</span><span class="n">layer_weights_shape</span><span class="p">))</span>
        <span class="n">weights_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">layer_weights_matrix</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">layer_weights_size</span>
    <span class="k">return</span> <span class="n">weights_dict</span>
</pre></div>
</div>
<p>With the helper functions we can now define the event handler for when the model receives a request to send the weights as well as when it gets new weights to update the model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">send_weights</span></code> handler extracts the model weights and send them back including the unique COMPONENT_ID.
While the <code class="docutils literal notranslate"><span class="pre">set_weights</span></code> handler updates the policy state dict with the received weights.
Both handler are triggered by DATA.CUSTOM messages in the <em>evolution</em> room, but one will only act with the command (CMD) GET in the message the other with the command (CMD) SET.</p>
<p>While the model did send the current weights to the evolutionary algorithm and until it receives new weights, we will set the training_enable variable to False.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">send_weights</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span><span class="n">policy</span><span class="p">,</span><span class="n">training_enabled</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;CMD&quot;</span> <span class="ow">in</span> <span class="n">msg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">msg</span><span class="p">[</span><span class="s2">&quot;CMD&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;GET&quot;</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">model_weights_as_vector</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
            <span class="n">training_enabled</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;WEIGHTS&quot;</span><span class="p">:</span> <span class="n">weights</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="s2">&quot;COMPONENT_ID&quot;</span><span class="p">:</span> <span class="n">COMPONENT_ID</span><span class="p">}</span>
<span class="n">client</span><span class="o">.</span><span class="n">add_eventHandler</span><span class="p">(</span><span class="n">send_weights</span><span class="p">,</span><span class="n">MESSAGE_TYPE</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CUSTOM</span><span class="p">,</span><span class="n">responseRooms</span><span class="o">=</span><span class="s1">&#39;evolution&#39;</span><span class="p">,</span><span class="n">trigger</span><span class="o">=</span><span class="n">Trigger</span><span class="p">(</span><span class="n">MESSAGE_TYPE</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CUSTOM</span><span class="p">,</span> <span class="s1">&#39;evolution&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span><span class="n">policy</span><span class="p">,</span><span class="n">training_enabled</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;CMD&quot;</span> <span class="ow">in</span> <span class="n">msg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">msg</span><span class="p">[</span><span class="s2">&quot;CMD&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;SET&quot;</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">msg</span><span class="p">[</span><span class="s2">&quot;WEIGHTS&quot;</span><span class="p">][</span><span class="n">COMPONENT_ID</span><span class="p">]</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_weights_as_dict</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">)))</span>
            <span class="n">training_enabled</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">client</span><span class="o">.</span><span class="n">add_eventHandler</span><span class="p">(</span><span class="n">set_weights</span><span class="p">,</span><span class="n">MESSAGE_TYPE</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CUSTOM</span><span class="p">,</span><span class="n">responseRooms</span><span class="o">=</span><span class="s1">&#39;evolution&#39;</span><span class="p">,</span><span class="n">trigger</span><span class="o">=</span><span class="n">Trigger</span><span class="p">(</span><span class="n">MESSAGE_TYPE</span><span class="o">.</span><span class="n">DATA</span><span class="o">.</span><span class="n">CUSTOM</span><span class="p">,</span> <span class="s1">&#39;evolution&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>After setting up all the required logic, we finally start our client to listen to new incoming messages.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">client</span><span class="o">.</span><span class="n">listen</span><span class="p">()</span>
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01_server.html" class="btn btn-neutral float-left" title="01_server.py" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="03_aggregation.html" class="btn btn-neutral float-right" title="03_aggregation.py" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, IshmaGurca.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>